{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rvb_K2y6Wnv0"
      ],
      "authorship_tag": "ABX9TyP9CM3Bh+LPYBNowKnm6c5J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcjazz/py-playground/blob/master/pytorchTuto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVAa789i4Xxl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Functions"
      ],
      "metadata": {
        "id": "8BrHB9m5Xm3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Squared error"
      ],
      "metadata": {
        "id": "pSyWt3cGYqZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Define the predicted and actual values as tensors\n",
        "predicted_tensor = torch.tensor([320000.0])\n",
        "actual_tensor = torch.tensor([300000.0])\n",
        "# Compute the MSE loss\n",
        "loss_value = loss_function(predicted_tensor, actual_tensor)\n",
        "print(loss_value.item()) # Loss value: 20000 * 20000 / 1 = ...\n",
        "# 400000000.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRZ3U8Ax5SU9",
        "outputId": "53e21354-808e-412f-d060-e29257f4322a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy Loss"
      ],
      "metadata": {
        "id": "zI4rnxoJYdKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Our dataset contains a single image of a dog, where\n",
        "# cat = 0 and dog = 1 (corresponding to index 0 and 1)\n",
        "target_tensor = torch.tensor([1])\n",
        "print(target_tensor)\n",
        "predicted_tensor = torch.tensor([[2.0, 5.0]])\n",
        "loss_value = loss_function(predicted_tensor, target_tensor)\n",
        "loss_value\n",
        "# tensor(0.0486)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJroIP8f5svX",
        "outputId": "cca4dd9c-4b52-42c0-baee-60a72c1aee26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0486)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_tensor = torch.tensor([[1.5, 1.1]])\n",
        "loss_value = loss_function(predicted_tensor, target_tensor)\n",
        "loss_value\n",
        "# tensor(0.9130)"
      ],
      "metadata": {
        "id": "uqBFzAzD64PO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c8a702-5937-46ea-f3e4-3a29f6b19501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9130)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Layer perceptron"
      ],
      "metadata": {
        "id": "Sy_auXtLXTt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden_layer = nn.Linear(input_size, 64)\n",
        "        self.output_layer = nn.Linear(64, 2)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.hidden_layer(x))\n",
        "        return self.output_layer(x)\n",
        "\n",
        "model = MLP(input_size=10)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cxj203AUg-H",
        "outputId": "923d4cf3-a34d-46cb-a204-82b029fa70f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (hidden_layer): Linear(in_features=10, out_features=64, bias=True)\n",
            "  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",
            "  (activation): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Opimizers"
      ],
      "metadata": {
        "id": "MWx4UDjwX3k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# momentum=0.9 smoothes out updates and can help training\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "j7Wy9JrJUk4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "rvb_K2y6Wnv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Create a toy dataset\n",
        "class NumberProductDataset(Dataset):\n",
        "    def __init__(self, data_range=(1, 10)):\n",
        "        self.numbers = list(range(data_range[0], data_range[1]))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        number1 = self.numbers[index]\n",
        "        number2 = self.numbers[index] + 1\n",
        "        return (number1, number2), number1 * number2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.numbers)\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = NumberProductDataset(\n",
        "    data_range=(0, 11)\n",
        ")\n",
        "\n",
        "# Access a data sample\n",
        "data_sample = dataset[3]\n",
        "print(data_sample)\n",
        "# ((3, 4), 12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzuS8NEoUyKK",
        "outputId": "b0c11adb-5537-41b8-f939-1d7717f37888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((3, 4), 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloaders"
      ],
      "metadata": {
        "id": "bw49ZAwBW0MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = NumberProductDataset(data_range=(0, 5))\n",
        "\n",
        "# Create a DataLoader instance\n",
        "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
        "\n",
        "# Iterating over batches\n",
        "for (num_pairs, products) in dataloader:\n",
        "    print(num_pairs, products)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg30xJgDW4RO",
        "outputId": "6f43e703-8f51-4461-face-671f20170ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([2, 3, 4]), tensor([3, 4, 5])] tensor([ 6, 12, 20])\n",
            "[tensor([1, 0]), tensor([2, 1])] tensor([2, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop (Full example)"
      ],
      "metadata": {
        "id": "YoHQoYcCZaEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Number Sum Dataset"
      ],
      "metadata": {
        "id": "5KuamK6Oajxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class NumberSumDataset(Dataset):\n",
        "    def __init__(self, data_range=(1, 10)):\n",
        "        self.numbers = list(range(data_range[0], data_range[1]))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        number1 = float(self.numbers[index // len(self.numbers)])\n",
        "        number2 = float(self.numbers[index % len(self.numbers)])\n",
        "        return torch.tensor([number1, number2]), torch.tensor([number1 + number2])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.numbers) ** 2"
      ],
      "metadata": {
        "id": "CkttS0QEW-Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect the Dataset"
      ],
      "metadata": {
        "id": "p80Q3M1oa3R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = NumberSumDataset(data_range=(1, 100))\n",
        "\n",
        "for i in range(5):\n",
        "    print(dataset[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5-McNmua1Uk",
        "outputId": "31bb45f7-9937-4842-d13d-89abc2ecedbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([1., 1.]), tensor([2.]))\n",
            "(tensor([1., 2.]), tensor([3.]))\n",
            "(tensor([1., 3.]), tensor([4.]))\n",
            "(tensor([1., 4.]), tensor([5.]))\n",
            "(tensor([1., 5.]), tensor([6.]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Model"
      ],
      "metadata": {
        "id": "PuB8TrfWbE88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden_layer = nn.Linear(input_size, 128)\n",
        "        self.output_layer = nn.Linear(128, 1)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.hidden_layer(x))\n",
        "        return self.output_layer(x)"
      ],
      "metadata": {
        "id": "T7F7-4ona85D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate Components Needed for Training"
      ],
      "metadata": {
        "id": "9TIO5pOnbOj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = NumberSumDataset(data_range=(0, 100))\n",
        "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "model = MLP(input_size=2)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "0Yds-J7EbM1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Training Loop"
      ],
      "metadata": {
        "id": "8KkdcXXfbqPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    total_loss = 0.0\n",
        "    for number_pairs, sums in dataloader:  # Iterate over the batches\n",
        "        predictions = model(number_pairs)  # Compute the model output\n",
        "        loss = loss_function(predictions, sums)  # Compute the loss\n",
        "        loss.backward()  # Perform backpropagation\n",
        "        optimizer.step()  # Update the parameters\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        total_loss += loss.item()  # Add the loss for all batches\n",
        "\n",
        "    # Print the loss for this epoch\n",
        "    print(\"Epoch {}: Sum of Batch Losses = {:.5f}\".format(epoch, total_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvWrvNiRbkVb",
        "outputId": "4393251d-03a4-4bc9-d10e-ed18eeaebe43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Sum of Batch Losses = 0.38632\n",
            "Epoch 1: Sum of Batch Losses = 0.44549\n",
            "Epoch 2: Sum of Batch Losses = 0.14930\n",
            "Epoch 3: Sum of Batch Losses = 0.14371\n",
            "Epoch 4: Sum of Batch Losses = 3.45584\n",
            "Epoch 5: Sum of Batch Losses = 0.26037\n",
            "Epoch 6: Sum of Batch Losses = 0.01100\n",
            "Epoch 7: Sum of Batch Losses = 0.01531\n",
            "Epoch 8: Sum of Batch Losses = 0.03548\n",
            "Epoch 9: Sum of Batch Losses = 0.09686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try the Model Out"
      ],
      "metadata": {
        "id": "FkAdb3Qvdcz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on 3 + 7\n",
        "model(torch.tensor([3.0, 7.0]))"
      ],
      "metadata": {
        "id": "r9z_9GJvbxvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd871ec9-93de-428e-ac34-69eff0ebc1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9.9783], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face"
      ],
      "metadata": {
        "id": "W9noplHJesd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# See how many tokens are in the vocabulary\n",
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc028vfqevpK",
        "outputId": "e0f66791-7fff-4a6b-ff95-50818f01fa05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sentence\n",
        "tokens = tokenizer.tokenize(\"I heart Generative AI\")\n",
        "\n",
        "# Print the tokens\n",
        "print(tokens)\n",
        "\n",
        "# Show the token ids assigned to each token\n",
        "print(tokenizer.convert_tokens_to_ids(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOSQBaw1fJpA",
        "outputId": "3df757db-f79c-4e1a-f2c6-1a6ca9388246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'heart', 'genera', '##tive', 'ai']\n",
            "[1045, 2540, 11416, 6024, 9932]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face Models Code example"
      ],
      "metadata": {
        "id": "_GznVWABfswW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Load a pre-trained sentiment analysis model\n",
        "model_name = \"textattack/bert-base-uncased-imdb\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Tokenize the input sequence\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "inputs = tokenizer(\"I love Generative AI\", return_tensors=\"pt\")\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs).logits\n",
        "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "    predicted_class = torch.argmax(probabilities)\n",
        "\n",
        "# Display sentiment result\n",
        "if predicted_class == 1:\n",
        "    print(f\"Sentiment: Positive ({probabilities[0][1] * 100:.2f}%)\")\n",
        "else:\n",
        "    print(f\"Sentiment: Negative ({probabilities[0][0] * 100:.2f}%)\")\n",
        "# Sentiment: Positive (88.68%)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL12c7ZBgNqu",
        "outputId": "044bd5b6-da81-45c7-c40a-53e2bd747e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Positive (88.68%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face Datasets"
      ],
      "metadata": {
        "id": "h5WmZkNzhjkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz9kXl_RiSy4",
        "outputId": "5f5d69f7-3f52-48d4-c3dd-ad7dce1503cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.1-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Load the IMDB dataset, which contains movie reviews\n",
        "# and sentiment labels (positive or negative)\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Fetch a revie from the training set\n",
        "review_number = 42\n",
        "sample_review = dataset[\"train\"][review_number]\n",
        "\n",
        "display(HTML(sample_review[\"text\"][:450] + \"...\"))\n",
        "# WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the\n",
        "# film.\n",
        "#\n",
        "# With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they\n",
        "# see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does.\n",
        "# Did they even think to check on the director's credentials...\n",
        "\n",
        "if sample_review[\"label\"] == 1:\n",
        "    print(\"Sentiment: Positive\")\n",
        "else:\n",
        "    print(\"Sentiment: Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "gyJi8xVHhhD1",
        "outputId": "8ee7220b-4283-4a2f-ae0c-75f0abba5124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the film.<br /><br />With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does. Did they even think to check on the director's credentials..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (DistilBertForSequenceClassification,\n",
        "    DistilBertTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=2\n",
        ")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=64,\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "sNGpZqe6h8bP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "cf9a85d4-45b6-41bc-9415-1f2f660ec8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkuidjamarco\u001b[0m (\u001b[33mkuidjamarco-kd-mark\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250218_032128-yd5w92xr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kuidjamarco-kd-mark/huggingface/runs/yd5w92xr' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/kuidjamarco-kd-mark/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/kuidjamarco-kd-mark/huggingface' target=\"_blank\">https://wandb.ai/kuidjamarco-kd-mark/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/kuidjamarco-kd-mark/huggingface/runs/yd5w92xr' target=\"_blank\">https://wandb.ai/kuidjamarco-kd-mark/huggingface/runs/yd5w92xr</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dCSP8hpLBiUQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}